{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6d45072-2954-49ce-ae6c-7eafc2e37edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸš€ Single Prediction Execution\n",
      "============================================================\n",
      "\n",
      "   â†’ Saved with predictions: results/mlp_3level\\1120_features_S2_yellow_predicted_3level.csv\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š Prediction Summary: 1120_features_S2_yellow_predicted_3level.csv\n",
      "============================================================\n",
      "\n",
      "[Overall Statistics]\n",
      "  Total windows: 736\n",
      "  Time range: 0.0s ~ 3685.0s\n",
      "\n",
      "[Probability Statistics]\n",
      "  S1 (Fluctuation) - ë…ë¦½ì  3-way classification:\n",
      "    Normal: mean=1.0000, std=0.0002\n",
      "    Yellow: mean=0.0000, std=0.0002\n",
      "    Red:    mean=0.0000, std=0.0000\n",
      "    Sum:    mean=1.0000 (should be 1.0)\n",
      "\n",
      "  S2 (Unbalance) - ë…ë¦½ì  3-way classification:\n",
      "    Normal: mean=0.0000, std=0.0000\n",
      "    Yellow: mean=0.9830, std=0.1085\n",
      "    Red:    mean=0.0170, std=0.1085\n",
      "    Sum:    mean=1.0000 (should be 1.0)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "MLP ê¸°ë°˜ 3ë‹¨ê³„ ì´ìƒ íƒì§€ ì‹œìŠ¤í…œ - ONNX ì˜ˆì¸¡/í‰ê°€ ìŠ¤í¬ë¦½íŠ¸\n",
    "\n",
    "ë…ë¦½ì ì¸ 3-way classification Ã— 2:\n",
    "- S1 (Fluctuation): [ì •ìƒ, í™©ìƒ‰, ì ìƒ‰] - softmax (í•©=1)\n",
    "- S2 (Unbalance):   [ì •ìƒ, í™©ìƒ‰, ì ìƒ‰] - softmax (í•©=1)\n",
    "\n",
    "3ë‹¨ê³„ ê²½ë³´:\n",
    "- ì •ìƒ (Normal): 0\n",
    "- í™©ìƒ‰ ê²½ë³´ (Yellow Alert): 1\n",
    "- ì ìƒ‰ ê²½ë³´ (Red Alert): 2\n",
    "\n",
    "ì¶œë ¥: [S1_ì •ìƒ, S1_í™©ìƒ‰, S1_ì ìƒ‰, S2_ì •ìƒ, S2_í™©ìƒ‰, S2_ì ìƒ‰]\n",
    "      â””â”€â”€â”€â”€â”€ softmax â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€ softmax â”€â”€â”€â”€â”€â”˜\n",
    "       (ë…ë¦½ì  S1 ë¶„ë¥˜)        (ë…ë¦½ì  S2 ë¶„ë¥˜)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "# ONNX Runtime\n",
    "import onnxruntime as ort\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# =====================================\n",
    "# 0. ì„¤ì •\n",
    "# =====================================\n",
    "OUTPUT_DIR_MODELS = \"models\"\n",
    "OUTPUT_DIR_RESULTS = \"results/mlp_3level\"\n",
    "\n",
    "ONNX_MODEL_PATH = os.path.join(OUTPUT_DIR_MODELS, \"mlp_classifier_3level.onnx\")\n",
    "SCALER_PATH = os.path.join(OUTPUT_DIR_MODELS, \"scaler_mlp_3level.pkl\")\n",
    "SPLIT_PATH = os.path.join(OUTPUT_DIR_MODELS, \"data_split_3level.npz\")\n",
    "\n",
    "# ê²½ë³´ ë ˆë²¨\n",
    "ALERT_LEVELS = ['Normal', 'Yellow', 'Red']\n",
    "ANOMALY_TYPES = ['S1_Fluctuation', 'S2_Unbalance']\n",
    "\n",
    "\n",
    "# =====================================\n",
    "# 1. ONNX ëª¨ë¸/ìŠ¤ì¼€ì¼ëŸ¬/ë°ì´í„° ë¡œë“œ\n",
    "# =====================================\n",
    "def load_onnx_model_scaler_and_split():\n",
    "    \"\"\"ONNX ëª¨ë¸, ìŠ¤ì¼€ì¼ëŸ¬, train/test ë¶„í•  ë°ì´í„°ë¥¼ ë¡œë“œ\"\"\"\n",
    "    if not os.path.exists(ONNX_MODEL_PATH):\n",
    "        raise FileNotFoundError(f\"ONNX model not found: {ONNX_MODEL_PATH}\")\n",
    "    if not os.path.exists(SCALER_PATH):\n",
    "        raise FileNotFoundError(f\"Scaler not found: {SCALER_PATH}\")\n",
    "    if not os.path.exists(SPLIT_PATH):\n",
    "        raise FileNotFoundError(f\"Data split not found: {SPLIT_PATH}\")\n",
    "\n",
    "    # ONNX Runtime ì„¸ì…˜ ìƒì„±\n",
    "    ort_session = ort.InferenceSession(ONNX_MODEL_PATH)\n",
    "\n",
    "    # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ\n",
    "    with open(SCALER_PATH, \"rb\") as f:\n",
    "        scaler = pickle.load(f)\n",
    "\n",
    "    # train/test ë¶„í•  ë°ì´í„° ë¡œë“œ\n",
    "    split = np.load(SPLIT_PATH)\n",
    "    X_train = split[\"X_train\"]\n",
    "    X_test = split[\"X_test\"]\n",
    "    y_train = split[\"y_train\"]\n",
    "    y_test = split[\"y_test\"]\n",
    "\n",
    "    return ort_session, scaler, X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# =====================================\n",
    "# 2. í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€\n",
    "# =====================================\n",
    "def evaluate_on_test_set():\n",
    "    \"\"\"í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì— ëŒ€í•œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ“‚ Loading ONNX model / scaler / data split\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    ort_session, scaler, X_train, X_test, y_train, y_test = load_onnx_model_scaler_and_split()\n",
    "\n",
    "    print(f\"âœ… ONNX model loaded: {ONNX_MODEL_PATH}\")\n",
    "    print(f\"âœ… X_train shape: {X_train.shape}\")\n",
    "    print(f\"âœ… X_test shape: {X_test.shape}\")\n",
    "    print(f\"âœ… y_train shape: {y_train.shape}\")\n",
    "    print(f\"âœ… y_test shape: {y_test.shape}\")\n",
    "\n",
    "    # ONNX Runtime ì¶”ë¡ \n",
    "    input_name = ort_session.get_inputs()[0].name\n",
    "    X_test_float32 = X_test.astype(np.float32)\n",
    "    ort_inputs = {input_name: X_test_float32}\n",
    "    \n",
    "    y_pred_proba = ort_session.run(None, ort_inputs)[0]  # (N, 6)\n",
    "\n",
    "    # S1, S2 í™•ë¥  ë¶„ë¦¬ (ë…ë¦½ì  ë¶„ë¥˜)\n",
    "    s1_probs = y_pred_proba[:, :3]  # (N, 3) - S1 ë…ë¦½ ë¶„ë¥˜\n",
    "    s2_probs = y_pred_proba[:, 3:]  # (N, 3) - S2 ë…ë¦½ ë¶„ë¥˜\n",
    "\n",
    "    # ì˜ˆì¸¡ ë ˆë²¨ (ê°ê° ë…ë¦½ì ìœ¼ë¡œ argmax)\n",
    "    s1_pred = np.argmax(s1_probs, axis=1)\n",
    "    s2_pred = np.argmax(s2_probs, axis=1)\n",
    "\n",
    "    # ì‹¤ì œ ë ˆë²¨\n",
    "    s1_true = np.argmax(y_test[:, :3], axis=1)\n",
    "    s2_true = np.argmax(y_test[:, 3:], axis=1)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ“ˆ Classification Report - S1 (Fluctuation)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(classification_report(s1_true, s1_pred, target_names=ALERT_LEVELS, digits=4, zero_division=0))\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ“ˆ Classification Report - S2 (Unbalance)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(classification_report(s2_true, s2_pred, target_names=ALERT_LEVELS, digits=4, zero_division=0))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ“Š Confusion Matrices\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    cm_s1 = confusion_matrix(s1_true, s1_pred)\n",
    "    cm_s2 = confusion_matrix(s2_true, s2_pred)\n",
    "\n",
    "    print(\"\\n[S1 Confusion Matrix]\")\n",
    "    print(\"              Predicted\")\n",
    "    print(\"         Normal Yellow   Red\")\n",
    "    for i, level in enumerate(ALERT_LEVELS):\n",
    "        print(f\"{level:8s} {cm_s1[i, 0]:6d} {cm_s1[i, 1]:6d} {cm_s1[i, 2]:6d}\")\n",
    "\n",
    "    print(\"\\n[S2 Confusion Matrix]\")\n",
    "    print(\"              Predicted\")\n",
    "    print(\"         Normal Yellow   Red\")\n",
    "    for i, level in enumerate(ALERT_LEVELS):\n",
    "        print(f\"{level:8s} {cm_s2[i, 0]:6d} {cm_s2[i, 1]:6d} {cm_s2[i, 2]:6d}\")\n",
    "\n",
    "    # ì „ì²´ ì •í™•ë„\n",
    "    s1_acc = np.mean(s1_true == s1_pred)\n",
    "    s2_acc = np.mean(s2_true == s2_pred)\n",
    "    overall_acc = np.mean((s1_true == s1_pred) & (s2_true == s2_pred))\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"âœ… Accuracy Summary\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  S1 (Fluctuation) accuracy: {s1_acc:.4f}\")\n",
    "    print(f\"  S2 (Unbalance) accuracy:   {s2_acc:.4f}\")\n",
    "    print(f\"  Overall accuracy (both correct): {overall_acc:.4f}\")\n",
    "\n",
    "    # ë©”íŠ¸ë¦­ ì €ì¥\n",
    "    os.makedirs(OUTPUT_DIR_RESULTS, exist_ok=True)\n",
    "\n",
    "    metrics = {\n",
    "        'model_type': 'MLP Classifier (3-Level Alert System) - ONNX',\n",
    "        'test_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'data_split': {\n",
    "            'train_samples': int(len(X_train)),\n",
    "            'test_samples': int(len(X_test)),\n",
    "        },\n",
    "        's1_metrics': {\n",
    "            'accuracy': float(s1_acc),\n",
    "            'confusion_matrix': cm_s1.tolist(),\n",
    "        },\n",
    "        's2_metrics': {\n",
    "            'accuracy': float(s2_acc),\n",
    "            'confusion_matrix': cm_s2.tolist(),\n",
    "        },\n",
    "        'overall_accuracy': float(overall_acc),\n",
    "    }\n",
    "\n",
    "    metrics_path = os.path.join(OUTPUT_DIR_RESULTS, 'mlp_3level_metrics.json')\n",
    "    with open(metrics_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metrics, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\nâœ… Metrics saved: {metrics_path}\")\n",
    "\n",
    "    # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥\n",
    "    pred_dump_path = os.path.join(OUTPUT_DIR_RESULTS, \"mlp_3level_test_predictions.npz\")\n",
    "    np.savez(\n",
    "        pred_dump_path,\n",
    "        y_test=y_test,\n",
    "        y_pred_proba=y_pred_proba,\n",
    "        s1_pred=s1_pred,\n",
    "        s2_pred=s2_pred,\n",
    "    )\n",
    "    print(f\"âœ… Predictions saved: {pred_dump_path}\")\n",
    "\n",
    "\n",
    "# =====================================\n",
    "# 3. CSV íŒŒì¼ ì˜ˆì¸¡\n",
    "# =====================================\n",
    "def predict_from_csv(csv_path: str, ort_session=None, scaler=None):\n",
    "    \"\"\"\n",
    "    íŠ¹ì§• ë²¡í„° CSVë¥¼ ì…ë ¥ë°›ì•„ 3ë‹¨ê³„ ê²½ë³´ ì˜ˆì¸¡ì„ ìˆ˜í–‰\n",
    "    \n",
    "    Args:\n",
    "        csv_path: window_id, start_time, end_time, feature1, feature2, ... êµ¬ì¡°\n",
    "        ort_session: ONNX Runtime ì„¸ì…˜\n",
    "        scaler: StandardScaler ê°ì²´\n",
    "    \n",
    "    Returns:\n",
    "        result_df: ì˜ˆì¸¡ ê²°ê³¼ê°€ í¬í•¨ëœ DataFrame\n",
    "    \"\"\"\n",
    "    if ort_session is None or scaler is None:\n",
    "        ort_session, scaler, _, _, _, _ = load_onnx_model_scaler_and_split()\n",
    "\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"CSV file not found: {csv_path}\")\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    if df.shape[1] <= 3:\n",
    "        raise ValueError(\"CSVì— feature ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    meta_cols = df.iloc[:, :3].copy()\n",
    "    X_raw = df.iloc[:, 3:].values\n",
    "\n",
    "    # íŠ¹ì§• ì •ê·œí™”\n",
    "    X_scaled = scaler.transform(X_raw)\n",
    "\n",
    "    # ONNX Runtimeìœ¼ë¡œ ì¶”ë¡ \n",
    "    input_name = ort_session.get_inputs()[0].name\n",
    "    X_float32 = X_scaled.astype(np.float32)\n",
    "    ort_inputs = {input_name: X_float32}\n",
    "    \n",
    "    y_proba = ort_session.run(None, ort_inputs)[0]  # (N, 6)\n",
    "\n",
    "    # S1, S2 í™•ë¥  ë¶„ë¦¬ (ë…ë¦½ì ì¸ 3-way classification)\n",
    "    s1_probs = y_proba[:, :3]  # (N, 3) - S1: [ì •ìƒ, í™©ìƒ‰, ì ìƒ‰]\n",
    "    s2_probs = y_proba[:, 3:]  # (N, 3) - S2: [ì •ìƒ, í™©ìƒ‰, ì ìƒ‰]\n",
    "    # ê° í™•ë¥  ë²¡í„°ì˜ í•©ì€ 1 (softmaxë¡œ ì •ê·œí™”ë¨)\n",
    "\n",
    "    # ê²°ê³¼ DataFrame êµ¬ì„±\n",
    "    result_df = meta_cols.copy()\n",
    "    \n",
    "    # S1 í™•ë¥ \n",
    "    result_df['s1_prob_normal'] = s1_probs[:, 0]\n",
    "    result_df['s1_prob_yellow'] = s1_probs[:, 1]\n",
    "    result_df['s1_prob_red'] = s1_probs[:, 2]\n",
    "    \n",
    "    # S2 í™•ë¥ \n",
    "    result_df['s2_prob_normal'] = s2_probs[:, 0]\n",
    "    result_df['s2_prob_yellow'] = s2_probs[:, 1]\n",
    "    result_df['s2_prob_red'] = s2_probs[:, 2]\n",
    "\n",
    "    # ì €ì¥\n",
    "    os.makedirs(OUTPUT_DIR_RESULTS, exist_ok=True)\n",
    "    \n",
    "    filename = os.path.basename(csv_path)\n",
    "    base, ext = os.path.splitext(filename)\n",
    "    out_path = os.path.join(OUTPUT_DIR_RESULTS, base + \"_predicted_3level\" + ext)\n",
    "    \n",
    "    result_df.to_csv(out_path, index=False)\n",
    "    print(f\"\\n   â†’ Saved with predictions: {out_path}\")\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "# =====================================\n",
    "# 4. ë°°ì¹˜ ì˜ˆì¸¡\n",
    "# =====================================\n",
    "def predict_batch_csvs(csv_paths: list):\n",
    "    \"\"\"ì—¬ëŸ¬ CSV íŒŒì¼ì— ëŒ€í•´ ë°°ì¹˜ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"ğŸ“‚ Batch prediction on {len(csv_paths)} CSV files\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ í•œ ë²ˆë§Œ ë¡œë“œ\n",
    "    ort_session, scaler, _, _, _, _ = load_onnx_model_scaler_and_split()\n",
    "\n",
    "    for csv_path in csv_paths:\n",
    "        try:\n",
    "            print(f\"\\nğŸ”® Processing: {csv_path}\")\n",
    "            predict_from_csv(csv_path, ort_session=ort_session, scaler=scaler)\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error processing {csv_path}: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"âœ… Batch prediction completed\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# =====================================\n",
    "# 5. í†µê³„ ìš”ì•½\n",
    "# =====================================\n",
    "def summarize_predictions(csv_path: str):\n",
    "    \"\"\"ì˜ˆì¸¡ ê²°ê³¼ CSVì˜ í†µê³„ ìš”ì•½ ì¶œë ¥\"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"Prediction CSV not found: {csv_path}\")\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"ğŸ“Š Prediction Summary: {os.path.basename(csv_path)}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # ì „ì²´ í†µê³„\n",
    "    print(f\"\\n[Overall Statistics]\")\n",
    "    print(f\"  Total windows: {len(df)}\")\n",
    "    print(f\"  Time range: {df['start_time'].min():.1f}s ~ {df['end_time'].max():.1f}s\")\n",
    "\n",
    "    # í™•ë¥  í†µê³„\n",
    "    print(f\"\\n[Probability Statistics]\")\n",
    "    print(f\"  S1 (Fluctuation) - ë…ë¦½ì  3-way classification:\")\n",
    "    print(f\"    Normal: mean={df['s1_prob_normal'].mean():.4f}, std={df['s1_prob_normal'].std():.4f}\")\n",
    "    print(f\"    Yellow: mean={df['s1_prob_yellow'].mean():.4f}, std={df['s1_prob_yellow'].std():.4f}\")\n",
    "    print(f\"    Red:    mean={df['s1_prob_red'].mean():.4f}, std={df['s1_prob_red'].std():.4f}\")\n",
    "    print(f\"    Sum:    mean={df[['s1_prob_normal', 's1_prob_yellow', 's1_prob_red']].sum(axis=1).mean():.4f} (should be 1.0)\")\n",
    "\n",
    "    print(f\"\\n  S2 (Unbalance) - ë…ë¦½ì  3-way classification:\")\n",
    "    print(f\"    Normal: mean={df['s2_prob_normal'].mean():.4f}, std={df['s2_prob_normal'].std():.4f}\")\n",
    "    print(f\"    Yellow: mean={df['s2_prob_yellow'].mean():.4f}, std={df['s2_prob_yellow'].std():.4f}\")\n",
    "    print(f\"    Red:    mean={df['s2_prob_red'].mean():.4f}, std={df['s2_prob_red'].std():.4f}\")\n",
    "    print(f\"    Sum:    mean={df[['s2_prob_normal', 's2_prob_yellow', 's2_prob_red']].sum(axis=1).mean():.4f} (should be 1.0)\")\n",
    "\n",
    "\n",
    "# =====================================\n",
    "# 6. main\n",
    "# =====================================\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # [Step 1] ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ (Sanity Check)\n",
    "    # ---------------------------------------------------------\n",
    "    \"\"\"\n",
    "    # ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê¸° ì „ì—, Test Setì— ëŒ€í•´ ëª¨ë¸ì´ ì •ìƒ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "    # í•„ìš” ì—†ë‹¤ë©´ ì£¼ì„ ì²˜ë¦¬ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "    try:\n",
    "        evaluate_on_test_set()\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[Warning] ëª¨ë¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (íŒŒì¼ ëˆ„ë½ ë“±): {e}\")\n",
    "        print(\"í‰ê°€ë¥¼ ê±´ë„ˆë›°ê³  ì˜ˆì¸¡ì„ ì§„í–‰í•©ë‹ˆë‹¤.\")\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # [Step 2] ë‹¨ì¼ CSV íŒŒì¼ ì˜ˆì¸¡ ì‹¤í–‰\n",
    "    # ---------------------------------------------------------\n",
    "    # ì˜ˆì¸¡í•  ëŒ€ìƒ íŒŒì¼ ê²½ë¡œ\n",
    "    target_csv = 'data/processed/1120_features_S2_yellow.csv'\n",
    "    \n",
    "    if os.path.exists(target_csv):\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ğŸš€ Single Prediction Execution\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # 1. ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "        predict_from_csv(target_csv)\n",
    "        \n",
    "        # 2. ê²°ê³¼ íŒŒì¼ ê²½ë¡œ ì¶”ë¡  (predict_from_csvì˜ ì €ì¥ ë¡œì§ê³¼ ë™ì¼í•˜ê²Œ êµ¬ì„±)\n",
    "        filename = os.path.basename(target_csv)\n",
    "        base, ext = os.path.splitext(filename)\n",
    "        predicted_csv_path = os.path.join(OUTPUT_DIR_RESULTS, base + \"_predicted_3level\" + ext)\n",
    "        \n",
    "        # 3. ê²°ê³¼ ìš”ì•½ ì¶œë ¥\n",
    "        if os.path.exists(predicted_csv_path):\n",
    "            summarize_predictions(predicted_csv_path)\n",
    "        else:\n",
    "            print(f\"âŒ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {predicted_csv_path}\")\n",
    "    else:\n",
    "        print(f\"\\n[Error] ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {target_csv}\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # [Step 3] ë°°ì¹˜ ì˜ˆì¸¡ (Batch Prediction) - ì£¼ì„ ì²˜ë¦¬ë¨\n",
    "    # ---------------------------------------------------------\n",
    "    \"\"\"\n",
    "    batch_datasets = [\n",
    "        'data/processed/1124_features.csv',\n",
    "        'data/processed/1120_features_S1_yellow.csv',\n",
    "        'data/processed/1120_features_S1.csv',\n",
    "        'data/processed/1120_features_S2_yellow.csv',\n",
    "        'data/processed/1120_features_S2.csv'\n",
    "    ]\n",
    "    \n",
    "    # 1. ì¡´ì¬í•˜ëŠ” íŒŒì¼ë§Œ í•„í„°ë§\n",
    "    valid_datasets = [f for f in batch_datasets if os.path.exists(f)]\n",
    "    \n",
    "    if valid_datasets:\n",
    "        # ì¼ê´„ ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "        predict_batch_csvs(valid_datasets)\n",
    "        \n",
    "        # ì¼ê´„ ìš”ì•½ ì¶œë ¥\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ğŸ“Š Batch Summary Reports\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for input_csv in valid_datasets:\n",
    "            # ê²°ê³¼ íŒŒì¼ ê²½ë¡œ ì¬êµ¬ì„±\n",
    "            fname = os.path.basename(input_csv)\n",
    "            fbase, fext = os.path.splitext(fname)\n",
    "            out_path = os.path.join(OUTPUT_DIR_RESULTS, fbase + \"_predicted_3level\" + fext)\n",
    "            \n",
    "            try:\n",
    "                summarize_predictions(out_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping summary for {fname}: {e}\")\n",
    "    else:\n",
    "        print(\"\\n[Batch] ì²˜ë¦¬í•  ìœ íš¨í•œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcb6589-b3b4-48ee-b81d-da12048412df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
