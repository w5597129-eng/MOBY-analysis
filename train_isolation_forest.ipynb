{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83eb43f0-ceb7-4113-9e0d-9289faeffd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ“‚ Loading Normal Data\n",
      "======================================================================\n",
      "âœ… Loaded 703 samples\n",
      "ğŸ“ Shape: (703, 80)\n",
      "\n",
      "âœ… Features: 77\n",
      "âœ… Training samples: (703, 77)\n",
      "\n",
      "ğŸ“Š Data Statistics:\n",
      "   Mean: 6965.9519\n",
      "   Std:  45002.2106\n",
      "   Min:  -9.8511\n",
      "   Max:  779501.1670\n",
      "\n",
      "======================================================================\n",
      "ğŸ”§ Data Normalization\n",
      "======================================================================\n",
      "âœ… Data normalized using StandardScaler\n",
      "   Scaled mean: 0.000000 (should be ~0)\n",
      "   Scaled std:  1.000000 (should be ~1)\n",
      "\n",
      "======================================================================\n",
      "ğŸŒ² Training Isolation Forest\n",
      "======================================================================\n",
      "Parameters:\n",
      "  contamination       : auto\n",
      "  n_estimators        : 100\n",
      "  max_samples         : auto\n",
      "  random_state        : 42\n",
      "  n_jobs              : -1\n",
      "  verbose             : 0\n",
      "\n",
      "âœ… Model training completed\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š Training Statistics\n",
      "======================================================================\n",
      "Training data predictions:\n",
      "  Normal (1):    693 (98.6%)\n",
      "  Anomaly (-1):    10 (1.4%)\n",
      "\n",
      "Anomaly scores:\n",
      "  Mean:   -0.432699\n",
      "  Std:    0.025405\n",
      "  Min:    -0.557497\n",
      "  Max:    -0.376018\n",
      "  Median: -0.430073\n",
      "\n",
      "======================================================================\n",
      "ğŸ’¾ Saving Model and Scaler\n",
      "======================================================================\n",
      "âœ… Model saved: models\\isolation_forest.joblib\n",
      "âœ… Scaler saved: models\\scaler_if.joblib\n",
      "âœ… Training info saved: results/isolation_forest\\training_info.json\n",
      "\n",
      "======================================================================\n",
      "âœ… Training Complete\n",
      "======================================================================\n",
      "\n",
      "[Saved Files]\n",
      "  â€¢ Model:        models\\isolation_forest.joblib\n",
      "  â€¢ Scaler:       models\\scaler_if.joblib\n",
      "  â€¢ Training info: results/isolation_forest\\training_info.json\n",
      "\n",
      "[Training Summary]\n",
      "  â€¢ Samples:      703\n",
      "  â€¢ Features:     77\n",
      "  â€¢ Normal pred:  693 (98.6%)\n",
      "  â€¢ Anomaly pred: 10 (1.4%)\n",
      "\n",
      "[Next Steps]\n",
      "  1. Use the model to predict on new data\n",
      "  2. Collect anomaly data for evaluation\n",
      "  3. Fine-tune contamination parameter if needed\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Isolation Forest í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (ì •ìƒ ë°ì´í„° ì „ìš©)\n",
    "- ì •ìƒ ë°ì´í„°ë¡œë§Œ í•™ìŠµ\n",
    "- ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥\n",
    "- ë¶ˆí•„ìš”í•œ ë¶„ì„/ì‹œê°í™” ì œì™¸\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# =====================================\n",
    "# ì„¤ì •\n",
    "# =====================================\n",
    "\n",
    "# ì…ë ¥ íŒŒì¼\n",
    "INPUT_FEATURES = 'data/processed/1118_features.csv'\n",
    "\n",
    "# ì¶œë ¥ ë””ë ‰í† ë¦¬\n",
    "OUTPUT_DIR_MODELS = 'models'\n",
    "OUTPUT_DIR_RESULTS = 'results/isolation_forest'\n",
    "\n",
    "# Isolation Forest íŒŒë¼ë¯¸í„°\n",
    "IF_PARAMS = {\n",
    "    'contamination': 'auto',  # ìë™ ì˜¤ì—¼ë„ ì¶”ì •\n",
    "    'n_estimators': 100,      # íŠ¸ë¦¬ ê°œìˆ˜\n",
    "    'max_samples': 'auto',    # ì„œë¸Œìƒ˜í”Œ í¬ê¸°\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,             # ëª¨ë“  CPU ì‚¬ìš©\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "os.makedirs(OUTPUT_DIR_MODELS, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR_RESULTS, exist_ok=True)\n",
    "\n",
    "# =====================================\n",
    "# 1. ë°ì´í„° ë¡œë“œ\n",
    "# =====================================\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ“‚ Loading Normal Data\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df = pd.read_csv(INPUT_FEATURES)\n",
    "print(f\"âœ… Loaded {len(df)} samples\")\n",
    "print(f\"ğŸ“ Shape: {df.shape}\")\n",
    "\n",
    "# =====================================\n",
    "# 2. íŠ¹ì§•ê³¼ ë©”íƒ€ë°ì´í„° ë¶„ë¦¬\n",
    "# =====================================\n",
    "\n",
    "# ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼ (ìœˆë„ìš° ì •ë³´)\n",
    "metadata_cols = ['window_id', 'start_time', 'end_time', 'n_samples']\n",
    "\n",
    "# íŠ¹ì§• ì»¬ëŸ¼ ì¶”ì¶œ\n",
    "feature_cols = [col for col in df.columns if col not in metadata_cols]\n",
    "\n",
    "# íŠ¹ì§• í–‰ë ¬\n",
    "X = df[feature_cols].values\n",
    "\n",
    "print(f\"\\nâœ… Features: {len(feature_cols)}\")\n",
    "print(f\"âœ… Training samples: {X.shape}\")\n",
    "\n",
    "# ê¸°ë³¸ í†µê³„\n",
    "print(f\"\\nğŸ“Š Data Statistics:\")\n",
    "print(f\"   Mean: {X.mean():.4f}\")\n",
    "print(f\"   Std:  {X.std():.4f}\")\n",
    "print(f\"   Min:  {X.min():.4f}\")\n",
    "print(f\"   Max:  {X.max():.4f}\")\n",
    "\n",
    "# =====================================\n",
    "# 3. ë°ì´í„° ì •ê·œí™”\n",
    "# =====================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ”§ Data Normalization\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"âœ… Data normalized using StandardScaler\")\n",
    "print(f\"   Scaled mean: {X_scaled.mean():.6f} (should be ~0)\")\n",
    "print(f\"   Scaled std:  {X_scaled.std():.6f} (should be ~1)\")\n",
    "\n",
    "# =====================================\n",
    "# 4. Isolation Forest í•™ìŠµ\n",
    "# =====================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸŒ² Training Isolation Forest\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"Parameters:\")\n",
    "for key, value in IF_PARAMS.items():\n",
    "    print(f\"  {key:20s}: {value}\")\n",
    "print()\n",
    "\n",
    "model = IsolationForest(**IF_PARAMS)\n",
    "model.fit(X_scaled)\n",
    "\n",
    "print(\"âœ… Model training completed\")\n",
    "\n",
    "# =====================================\n",
    "# 5. í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ ê¸°ë³¸ í†µê³„\n",
    "# =====================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š Training Statistics\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ì´ìƒ ì ìˆ˜ ê³„ì‚°\n",
    "anomaly_scores = model.score_samples(X_scaled)\n",
    "predictions = model.predict(X_scaled)\n",
    "\n",
    "# í†µê³„\n",
    "n_normal = np.sum(predictions == 1)\n",
    "n_anomaly = np.sum(predictions == -1)\n",
    "\n",
    "print(f\"Training data predictions:\")\n",
    "print(f\"  Normal (1):  {n_normal:5d} ({n_normal/len(predictions)*100:.1f}%)\")\n",
    "print(f\"  Anomaly (-1): {n_anomaly:5d} ({n_anomaly/len(predictions)*100:.1f}%)\")\n",
    "print(f\"\\nAnomaly scores:\")\n",
    "print(f\"  Mean:   {anomaly_scores.mean():.6f}\")\n",
    "print(f\"  Std:    {anomaly_scores.std():.6f}\")\n",
    "print(f\"  Min:    {anomaly_scores.min():.6f}\")\n",
    "print(f\"  Max:    {anomaly_scores.max():.6f}\")\n",
    "print(f\"  Median: {np.median(anomaly_scores):.6f}\")\n",
    "\n",
    "# =====================================\n",
    "# 6. ëª¨ë¸ ì €ì¥\n",
    "# =====================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ’¾ Saving Model and Scaler\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥\n",
    "model_path = os.path.join(OUTPUT_DIR_MODELS, 'isolation_forest.joblib')\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "print(f\"âœ… Model saved: {model_path}\")\n",
    "\n",
    "# ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥\n",
    "scaler_path = os.path.join(OUTPUT_DIR_MODELS, 'scaler_if.joblib')\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"âœ… Scaler saved: {scaler_path}\")\n",
    "\n",
    "# =====================================\n",
    "# 7. í•™ìŠµ ì •ë³´ ì €ì¥ (JSON)\n",
    "# =====================================\n",
    "training_info = {\n",
    "    'model_type': 'Isolation Forest',\n",
    "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'data_info': {\n",
    "        'input_file': INPUT_FEATURES,\n",
    "        'n_samples': int(len(X)),\n",
    "        'n_features': int(len(feature_cols)),\n",
    "        'feature_cols': feature_cols\n",
    "    },\n",
    "    'model_params': IF_PARAMS,\n",
    "    'training_stats': {\n",
    "        'normal_predictions': int(n_normal),\n",
    "        'anomaly_predictions': int(n_anomaly),\n",
    "        'anomaly_ratio': float(n_anomaly / len(predictions)),\n",
    "        'score_mean': float(anomaly_scores.mean()),\n",
    "        'score_std': float(anomaly_scores.std()),\n",
    "        'score_min': float(anomaly_scores.min()),\n",
    "        'score_max': float(anomaly_scores.max()),\n",
    "        'score_median': float(np.median(anomaly_scores))\n",
    "    },\n",
    "    'scaler_info': {\n",
    "        'type': 'StandardScaler',\n",
    "        'feature_mean': scaler.mean_.tolist(),\n",
    "        'feature_std': scaler.scale_.tolist()\n",
    "    }\n",
    "}\n",
    "\n",
    "info_path = os.path.join(OUTPUT_DIR_RESULTS, 'training_info.json')\n",
    "with open(info_path, 'w') as f:\n",
    "    json.dump(training_info, f, indent=4)\n",
    "print(f\"âœ… Training info saved: {info_path}\")\n",
    "\n",
    "# =====================================\n",
    "# 8. ìš”ì•½\n",
    "# =====================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… Training Complete\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n[Saved Files]\")\n",
    "print(f\"  â€¢ Model:        {model_path}\")\n",
    "print(f\"  â€¢ Scaler:       {scaler_path}\")\n",
    "print(f\"  â€¢ Training info: {info_path}\")\n",
    "print(f\"\\n[Training Summary]\")\n",
    "print(f\"  â€¢ Samples:      {len(X)}\")\n",
    "print(f\"  â€¢ Features:     {len(feature_cols)}\")\n",
    "print(f\"  â€¢ Normal pred:  {n_normal} ({n_normal/len(predictions)*100:.1f}%)\")\n",
    "print(f\"  â€¢ Anomaly pred: {n_anomaly} ({n_anomaly/len(predictions)*100:.1f}%)\")\n",
    "print(f\"\\n[Next Steps]\")\n",
    "print(f\"  1. Use the model to predict on new data\")\n",
    "print(f\"  2. Collect anomaly data for evaluation\")\n",
    "print(f\"  3. Fine-tune contamination parameter if needed\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a03a367-71f1-492a-99ee-46021ffea068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
