{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Feature Extraction V17 - PCA + Vector Scalarization\n",
      "======================================================================\n",
      "\n",
      "=== Multi-Sensor Processing V17 (PCA + Vector Scalarization) ===\n",
      "Expected features: 12 (Accel: 9, Gyro: 1, Env: 2)\n",
      "  accel_gyro: 22124 samples @ 6.22 Hz\n",
      "  pressure: 22124 samples @ 6.22 Hz\n",
      "\n",
      "Synchronizing at 78.125ms...\n",
      "Synchronized: 45510 samples\n",
      "\n",
      "Extracting V17 features with 10.0s windows (overlap: 5.0s)...\n",
      "Effective sampling rate: 12.80 Hz\n",
      "Extracted features from 710 windows\n",
      "Total feature columns: 12\n",
      "\n",
      "✓ Saved to: data/processed\\1120_features.csv\n",
      "✓ Shape: (710, 15)\n",
      "✓ Features per window: 12\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Feature Extraction from Sensor Data (InfluxDB Format)\n",
    "Version 17 (PCA + Vector Scalarization + FFT Reuse):\n",
    "- Vector scalarization: 3-axis → scalar features\n",
    "- PCA-based dimensionality reduction\n",
    "- FFT reuse optimization\n",
    "- Maximum computational efficiency\n",
    "\n",
    "Key Improvements over V16:\n",
    "- Feature count: 23 → 12 (47.8% reduction)\n",
    "- FFT calls: 6 → 1 (83.3% reduction!) ⭐\n",
    "- Speed: ~60-65% faster (FFT-weighted)\n",
    "\n",
    "Optimizations:\n",
    "- PCA: 3 axes → 1 principal component (67% FFT reduction)\n",
    "- FFT reuse: Dominant Freq + RMSF share same spectrum (50% additional)\n",
    "- Vector RMS: Eliminated redundant sqrt operations\n",
    "\n",
    "Version 16 (Optimized):\n",
    "- Sensor-specific feature extraction (only extract what's needed)\n",
    "- Added RMS Frequency (RMSF) feature\n",
    "- Reduced feature count from 77 to 23\n",
    "\n",
    "Author: WISE Team, Project MOBY\n",
    "Date: 2025-11-21 (Optimized)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.fft import rfft, rfftfreq\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =====================================\n",
    "# 설정\n",
    "# =====================================\n",
    "\n",
    "# 센서별 추출할 특징 설정 (V17)\n",
    "FEATURE_CONFIG_V17 = {\n",
    "    # 3축 가속도: 9개 특징 (벡터 스칼라화 + PCA)\n",
    "    'accel': [\n",
    "        'VectorRMS',          # 1. 전체 진동 에너지\n",
    "        'PC1_PeakToPeak',     # 2. 주축 최대 진폭\n",
    "        'VectorCrestFactor',  # 3. 충격도 (vector norm 기반)\n",
    "        'PC1_DominantFreq',   # 4. 주축 주파수\n",
    "        'PC1_RMSF',           # 5. 주축 고주파 이동\n",
    "        'PC1_VarianceRatio',  # 6. 주축 설명력 (단일축 지배도)\n",
    "        'PC1_Direction_X',    # 7. 주축 방향 X 성분\n",
    "        'PC1_Direction_Y',    # 8. 주축 방향 Y 성분\n",
    "        'PC1_Direction_Z'     # 9. 주축 방향 Z 성분\n",
    "    ],\n",
    "    \n",
    "    # 3축 각속도: 1개 특징 (벡터 스칼라화)\n",
    "    'gyro': [\n",
    "        'VectorRMS'           # 1. 속도 불안정성 총량\n",
    "    ],\n",
    "    \n",
    "    # 환경 센서: 2개 특징\n",
    "    'pressure': ['Mean'],\n",
    "    'temperature': ['Mean']\n",
    "}\n",
    "\n",
    "# 윈도우 설정\n",
    "WINDOW_SIZE = 10.0  # 초 단위\n",
    "WINDOW_OVERLAP = 5.0  # 초 단위\n",
    "\n",
    "# 출력 디렉토리\n",
    "OUTPUT_DIR = 'data/processed'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# =====================================\n",
    "# CSV 읽기 함수\n",
    "# =====================================\n",
    "\n",
    "def read_influxdb_csv(file_path: str) -> Tuple[pd.DataFrame, float]:\n",
    "    \"\"\"\n",
    "    InfluxDB CSV 파일 읽기 (V16과 동일)\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    header_lines = [i for i, line in enumerate(lines) if line.startswith('#group')]\n",
    "    \n",
    "    if len(header_lines) > 1:\n",
    "        all_dfs = []\n",
    "        for idx, header_start in enumerate(header_lines):\n",
    "            if idx + 1 < len(header_lines):\n",
    "                section_end = header_lines[idx + 1]\n",
    "            else:\n",
    "                section_end = len(lines)\n",
    "            \n",
    "            from io import StringIO\n",
    "            section_text = ''.join(lines[header_start:section_end])\n",
    "            \n",
    "            try:\n",
    "                df_section = pd.read_csv(StringIO(section_text), skiprows=3, \n",
    "                                        dtype={'_time': str, '_value': float, '_field': str})\n",
    "                all_dfs.append(df_section)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if all_dfs:\n",
    "            df = pd.concat(all_dfs, ignore_index=True)\n",
    "        else:\n",
    "            raise ValueError(\"Could not read any valid sections\")\n",
    "    else:\n",
    "        df = pd.read_csv(file_path, skiprows=3, \n",
    "                         dtype={'_time': str, '_field': str},\n",
    "                         low_memory=False)\n",
    "    \n",
    "    df = df[df['_time'].notna() & (df['_time'] != '_time')]\n",
    "    df = df[df['_field'].notna() & (df['_field'] != '_field')]\n",
    "    df['_value'] = pd.to_numeric(df['_value'], errors='coerce')\n",
    "    df = df[df['_value'].notna()]\n",
    "    \n",
    "    df_pivot = df.pivot_table(\n",
    "        index='_time',\n",
    "        columns='_field',\n",
    "        values='_value',\n",
    "        aggfunc='first'\n",
    "    ).reset_index()\n",
    "    \n",
    "    df_pivot['_time'] = pd.to_datetime(df_pivot['_time'], format='mixed', utc=True)\n",
    "    df_pivot = df_pivot.sort_values('_time').reset_index(drop=True)\n",
    "    df_pivot['Time(s)'] = (df_pivot['_time'] - df_pivot['_time'].iloc[0]).dt.total_seconds()\n",
    "    \n",
    "    n_samples = len(df_pivot)\n",
    "    total_time = df_pivot['Time(s)'].iloc[-1] - df_pivot['Time(s)'].iloc[0]\n",
    "    sampling_rate = (n_samples - 1) / total_time if total_time > 0 else 1.0\n",
    "    \n",
    "    return df_pivot, sampling_rate\n",
    "\n",
    "# =====================================\n",
    "# PCA 함수\n",
    "# =====================================\n",
    "\n",
    "def compute_pca(data_3axis: np.ndarray) -> Dict:\n",
    "    \"\"\"\n",
    "    3축 데이터에 대한 PCA 수행\n",
    "    \n",
    "    Parameters:\n",
    "    - data_3axis: (n_samples, 3) shape의 numpy array\n",
    "    \n",
    "    Returns:\n",
    "    - dict with keys:\n",
    "        - 'pc1': First principal component (n_samples,)\n",
    "        - 'variance_ratio': Explained variance ratio of PC1\n",
    "        - 'direction': PC1 direction vector (3,)\n",
    "        - 'centered_data': Centered data (n_samples, 3)\n",
    "    \"\"\"\n",
    "    if len(data_3axis) < 3:\n",
    "        return {\n",
    "            'pc1': np.zeros(len(data_3axis)),\n",
    "            'variance_ratio': 0.0,\n",
    "            'direction': np.array([0.0, 0.0, 0.0]),\n",
    "            'centered_data': data_3axis\n",
    "        }\n",
    "    \n",
    "    # 평균 제거 (centering)\n",
    "    mean = np.mean(data_3axis, axis=0)\n",
    "    centered_data = data_3axis - mean\n",
    "    \n",
    "    # 공분산 행렬\n",
    "    cov_matrix = np.cov(centered_data, rowvar=False)\n",
    "    \n",
    "    # 고유값, 고유벡터 계산\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "    \n",
    "    # 고유값 내림차순 정렬\n",
    "    idx = eigenvalues.argsort()[::-1]\n",
    "    eigenvalues = eigenvalues[idx]\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "    \n",
    "    # PC1 (첫 번째 주성분)\n",
    "    pc1_direction = eigenvectors[:, 0]\n",
    "    pc1_data = centered_data @ pc1_direction\n",
    "    \n",
    "    # PC1 설명력 (분산 비율)\n",
    "    total_variance = np.sum(eigenvalues)\n",
    "    variance_ratio = eigenvalues[0] / total_variance if total_variance > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'pc1': pc1_data,\n",
    "        'variance_ratio': variance_ratio,\n",
    "        'direction': pc1_direction,\n",
    "        'centered_data': centered_data\n",
    "    }\n",
    "\n",
    "# =====================================\n",
    "# 벡터 특징 함수들\n",
    "# =====================================\n",
    "\n",
    "def compute_vector_rms(data_3axis: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Vector RMS: sqrt(mean(||v||²))\n",
    "    \n",
    "    Optimized: Eliminates unnecessary sqrt → square cancellation\n",
    "    \n",
    "    물리적 의미: 3축의 총 진동 에너지 (방향 무관)\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.mean(np.sum(data_3axis ** 2, axis=1)))\n",
    "\n",
    "def compute_vector_crest_factor(data_3axis: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Vector Crest Factor: max(||v||) / RMS(||v||)\n",
    "    \n",
    "    물리적 의미: 벡터 크기의 충격도 (축 독립적)\n",
    "    \"\"\"\n",
    "    vector_magnitude = np.sqrt(np.sum(data_3axis ** 2, axis=1))\n",
    "    peak = np.max(vector_magnitude)\n",
    "    rms = np.sqrt(np.mean(vector_magnitude ** 2))\n",
    "    return peak / rms if rms > 0 else 0.0\n",
    "\n",
    "def compute_pc1_peak_to_peak(pc1_data: np.ndarray) -> float:\n",
    "    \"\"\"PC1 축의 Peak-to-Peak\"\"\"\n",
    "    return np.ptp(pc1_data)\n",
    "\n",
    "def compute_pc1_freq_features(pc1_data: np.ndarray, sampling_rate: float) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    PC1 축의 주파수 특징 (Dominant Freq + RMSF)\n",
    "    \n",
    "    FFT를 한 번만 수행하고 두 특징을 모두 계산\n",
    "    \n",
    "    Parameters:\n",
    "    - pc1_data: PC1 시계열 데이터\n",
    "    - sampling_rate: 샘플링 주파수\n",
    "    \n",
    "    Returns:\n",
    "    - (dominant_freq, rmsf) tuple\n",
    "    \n",
    "    Optimization: 기존에는 FFT를 2번 호출했지만,\n",
    "                  같은 스펙트럼을 공유하므로 1번만 호출!\n",
    "    \"\"\"\n",
    "    N = len(pc1_data)\n",
    "    if N < 2:\n",
    "        return 0.0, 0.0\n",
    "    \n",
    "    # DC 제거\n",
    "    signal = pc1_data - np.mean(pc1_data)\n",
    "    \n",
    "    # === FFT 한 번만! ===\n",
    "    spectrum = np.abs(rfft(signal))\n",
    "    freqs = rfftfreq(N, 1/sampling_rate)\n",
    "    \n",
    "    # === Dominant Frequency (스펙트럼 재사용) ===\n",
    "    dominant_freq = freqs[np.argmax(spectrum)] if len(spectrum) > 0 else 0.0\n",
    "    \n",
    "    # === RMSF (스펙트럼 재사용) ===\n",
    "    power = spectrum ** 2\n",
    "    numerator = np.sum((freqs ** 2) * power)\n",
    "    denominator = np.sum(power)\n",
    "    rmsf = np.sqrt(numerator / denominator) if denominator > 0 else 0.0\n",
    "    \n",
    "    return dominant_freq, rmsf\n",
    "\n",
    "def compute_mean(signal: np.ndarray) -> float:\n",
    "    \"\"\"Mean\"\"\"\n",
    "    return np.mean(signal)\n",
    "\n",
    "# =====================================\n",
    "# 통합 특징 추출 함수\n",
    "# =====================================\n",
    "\n",
    "def extract_features_v17(data_dict: Dict[str, np.ndarray], \n",
    "                         sampling_rate: float) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    V17 특징 추출: PCA + 벡터 스칼라화\n",
    "    \n",
    "    Parameters:\n",
    "    - data_dict: {\n",
    "        'accel': (n, 3) numpy array,\n",
    "        'gyro': (n, 3) numpy array,\n",
    "        'pressure': (n,) numpy array,\n",
    "        'temperature': (n,) numpy array\n",
    "      }\n",
    "    - sampling_rate: 샘플링 주파수\n",
    "    \n",
    "    Returns:\n",
    "    - features: {feature_name: value} 딕셔너리\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # ===== 가속도 특징 (9개) =====\n",
    "    if 'accel' in data_dict and len(data_dict['accel']) > 0:\n",
    "        accel_data = data_dict['accel']\n",
    "        \n",
    "        # PCA 수행\n",
    "        pca_result = compute_pca(accel_data)\n",
    "        \n",
    "        # 1. Vector RMS\n",
    "        features['accel_VectorRMS'] = compute_vector_rms(accel_data)\n",
    "        \n",
    "        # 2. PC1 Peak-to-Peak\n",
    "        features['accel_PC1_PeakToPeak'] = compute_pc1_peak_to_peak(pca_result['pc1'])\n",
    "        \n",
    "        # 3. Vector Crest Factor\n",
    "        features['accel_VectorCrestFactor'] = compute_vector_crest_factor(accel_data)\n",
    "        \n",
    "        # 4-5. PC1 Dominant Frequency + RMSF (FFT 한 번에 둘 다!)\n",
    "        dominant_freq, rmsf = compute_pc1_freq_features(pca_result['pc1'], sampling_rate)\n",
    "        features['accel_PC1_DominantFreq'] = dominant_freq\n",
    "        features['accel_PC1_RMSF'] = rmsf\n",
    "        \n",
    "        # 6. PC1 Variance Ratio (설명력)\n",
    "        features['accel_PC1_VarianceRatio'] = pca_result['variance_ratio']\n",
    "        \n",
    "        # 7-9. PC1 Direction (방향 벡터)\n",
    "        features['accel_PC1_Direction_X'] = pca_result['direction'][0]\n",
    "        features['accel_PC1_Direction_Y'] = pca_result['direction'][1]\n",
    "        features['accel_PC1_Direction_Z'] = pca_result['direction'][2]\n",
    "    \n",
    "    # ===== 각속도 특징 (1개) =====\n",
    "    if 'gyro' in data_dict and len(data_dict['gyro']) > 0:\n",
    "        gyro_data = data_dict['gyro']\n",
    "        \n",
    "        # Vector RMS (Mean과 STD를 대체)\n",
    "        features['gyro_VectorRMS'] = compute_vector_rms(gyro_data)\n",
    "    \n",
    "    # ===== 환경 특징 (2개) =====\n",
    "    if 'pressure' in data_dict and len(data_dict['pressure']) > 0:\n",
    "        features['pressure_Mean'] = compute_mean(data_dict['pressure'])\n",
    "    \n",
    "    if 'temperature' in data_dict and len(data_dict['temperature']) > 0:\n",
    "        features['temperature_Mean'] = compute_mean(data_dict['temperature'])\n",
    "    \n",
    "    return features\n",
    "\n",
    "# =====================================\n",
    "# 다중 센서 파일 처리\n",
    "# =====================================\n",
    "\n",
    "def process_multi_sensor_files_v17(file_dict: Dict[str, str],\n",
    "                                    resample_rate: str = '100ms',\n",
    "                                    window_size: float = WINDOW_SIZE,\n",
    "                                    window_overlap: float = WINDOW_OVERLAP) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    여러 센서 파일을 동기화하여 V17 특징 추출\n",
    "    \n",
    "    Parameters:\n",
    "    - file_dict: {sensor_type: file_path} 딕셔너리\n",
    "    - resample_rate: 동기화 시 리샘플링 주기\n",
    "    - window_size: 윈도우 크기 (초)\n",
    "    - window_overlap: 윈도우 겹침 (초)\n",
    "    \n",
    "    Returns:\n",
    "    - 특징 DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n=== Multi-Sensor Processing V17 (PCA + Vector Scalarization) ===\")\n",
    "    print(f\"Expected features: 12 (Accel: 9, Gyro: 1, Env: 2)\")\n",
    "    \n",
    "    # 1. 각 센서 파일 독립적으로 읽고 리샘플링\n",
    "    resampled_dfs = []\n",
    "    sensor_info = []\n",
    "    \n",
    "    sensor_fields = {\n",
    "        'accel_gyro': ['fields_accel_x', 'fields_accel_y', 'fields_accel_z',\n",
    "                       'fields_gyro_x', 'fields_gyro_y', 'fields_gyro_z'],\n",
    "        'pressure': ['fields_pressure_hpa', 'fields_temperature_c']\n",
    "    }\n",
    "    \n",
    "    for sensor_type, file_path in file_dict.items():\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                df, sr = read_influxdb_csv(file_path)\n",
    "                sensor_info.append(f\"{sensor_type}: {len(df)} samples @ {sr:.2f} Hz\")\n",
    "                \n",
    "                df_indexed = df.set_index('_time')\n",
    "                \n",
    "                # 해당 센서의 필드들 선택\n",
    "                available_fields = [col for col in df_indexed.columns \n",
    "                                   if col in sensor_fields.get(sensor_type, [])]\n",
    "                \n",
    "                if available_fields:\n",
    "                    df_resampled = df_indexed[available_fields].resample(resample_rate).mean()\n",
    "                    resampled_dfs.append(df_resampled)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  Error reading {sensor_type}: {e}\")\n",
    "    \n",
    "    for info in sensor_info:\n",
    "        print(f\"  {info}\")\n",
    "    \n",
    "    if not resampled_dfs:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # 2. Outer join으로 병합\n",
    "    print(f\"\\nSynchronizing at {resample_rate}...\")\n",
    "    \n",
    "    merged_df = resampled_dfs[0].copy()\n",
    "    for df in resampled_dfs[1:]:\n",
    "        merged_df = merged_df.join(df, how='outer')\n",
    "    \n",
    "    # 3. NaN 보간\n",
    "    merged_df = merged_df.ffill().bfill()\n",
    "    \n",
    "    # 4. 상대 시간 추가\n",
    "    merged_df = merged_df.reset_index()\n",
    "    merged_df['Time(s)'] = (merged_df['_time'] - merged_df['_time'].iloc[0]).dt.total_seconds()\n",
    "    \n",
    "    print(f\"Synchronized: {len(merged_df)} samples\")\n",
    "    \n",
    "    # 5. 윈도우 기반 특징 추출\n",
    "    window_step = window_size - window_overlap\n",
    "    \n",
    "    n_samples = len(merged_df)\n",
    "    total_time = merged_df['Time(s)'].iloc[-1] - merged_df['Time(s)'].iloc[0]\n",
    "    effective_sr = (n_samples - 1) / total_time if total_time > 0 else 1.0\n",
    "    \n",
    "    print(f\"\\nExtracting V17 features with {window_size}s windows (overlap: {window_overlap}s)...\")\n",
    "    print(f\"Effective sampling rate: {effective_sr:.2f} Hz\")\n",
    "    \n",
    "    features_list = []\n",
    "    \n",
    "    start_time = merged_df['Time(s)'].iloc[0]\n",
    "    end_time = merged_df['Time(s)'].iloc[-1]\n",
    "    \n",
    "    current_time = start_time\n",
    "    window_count = 0\n",
    "    \n",
    "    while current_time + window_size <= end_time:\n",
    "        window_end = current_time + window_size\n",
    "        \n",
    "        # 윈도우 데이터 추출\n",
    "        window_mask = (merged_df['Time(s)'] >= current_time) & (merged_df['Time(s)'] < window_end)\n",
    "        window_data = merged_df[window_mask]\n",
    "        \n",
    "        if len(window_data) < 2:\n",
    "            current_time += window_step\n",
    "            continue\n",
    "        \n",
    "        # 데이터 준비\n",
    "        data_dict = {}\n",
    "        \n",
    "        # 가속도 3축\n",
    "        accel_cols = ['fields_accel_x', 'fields_accel_y', 'fields_accel_z']\n",
    "        if all(col in window_data.columns for col in accel_cols):\n",
    "            accel_3axis = window_data[accel_cols].values\n",
    "            # NaN 제거\n",
    "            valid_mask = ~np.isnan(accel_3axis).any(axis=1)\n",
    "            if valid_mask.sum() > 0:\n",
    "                data_dict['accel'] = accel_3axis[valid_mask]\n",
    "        \n",
    "        # 각속도 3축\n",
    "        gyro_cols = ['fields_gyro_x', 'fields_gyro_y', 'fields_gyro_z']\n",
    "        if all(col in window_data.columns for col in gyro_cols):\n",
    "            gyro_3axis = window_data[gyro_cols].values\n",
    "            valid_mask = ~np.isnan(gyro_3axis).any(axis=1)\n",
    "            if valid_mask.sum() > 0:\n",
    "                data_dict['gyro'] = gyro_3axis[valid_mask]\n",
    "        \n",
    "        # 환경\n",
    "        if 'fields_pressure_hpa' in window_data.columns:\n",
    "            pressure = window_data['fields_pressure_hpa'].values\n",
    "            pressure = pressure[~np.isnan(pressure)]\n",
    "            if len(pressure) > 0:\n",
    "                data_dict['pressure'] = pressure\n",
    "        \n",
    "        if 'fields_temperature_c' in window_data.columns:\n",
    "            temperature = window_data['fields_temperature_c'].values\n",
    "            temperature = temperature[~np.isnan(temperature)]\n",
    "            if len(temperature) > 0:\n",
    "                data_dict['temperature'] = temperature\n",
    "        \n",
    "        # 특징 추출\n",
    "        features = extract_features_v17(data_dict, effective_sr)\n",
    "        \n",
    "        # 메타데이터 추가\n",
    "        features['window_id'] = window_count\n",
    "        features['start_time'] = current_time\n",
    "        features['end_time'] = window_end\n",
    "        \n",
    "        features_list.append(features)\n",
    "        window_count += 1\n",
    "        current_time += window_step\n",
    "    \n",
    "    print(f\"Extracted features from {window_count} windows\")\n",
    "    \n",
    "    result_df = pd.DataFrame(features_list)\n",
    "    \n",
    "    # 컬럼 순서 정리 (메타데이터 먼저)\n",
    "    meta_cols = ['window_id', 'start_time', 'end_time']\n",
    "    feature_cols = [col for col in result_df.columns if col not in meta_cols]\n",
    "    result_df = result_df[meta_cols + feature_cols]\n",
    "    \n",
    "    print(f\"Total feature columns: {len(feature_cols)}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# =====================================\n",
    "# 메인 실행\n",
    "# =====================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Feature Extraction V17 - PCA + Vector Scalarization\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    sensor_files = {\n",
    "        'accel_gyro': 'data/raw/1120 sensor_data/1120_accel_gyro_normal.csv',\n",
    "        'pressure': 'data/raw/1120 sensor_data/1120_pressure_normal.csv',\n",
    "    }\n",
    "    \n",
    "    valid_files = {k: v for k, v in sensor_files.items() if os.path.exists(v)}\n",
    "    \n",
    "    if valid_files:\n",
    "        features = process_multi_sensor_files_v17(\n",
    "            valid_files,\n",
    "            resample_rate='78.125ms',  # ~12.8Hz\n",
    "            window_size=WINDOW_SIZE,\n",
    "            window_overlap=WINDOW_OVERLAP\n",
    "        )\n",
    "        \n",
    "        if not features.empty:\n",
    "            output_path = os.path.join(OUTPUT_DIR, \"1120_features.csv\")\n",
    "            features.to_csv(output_path, index=False)\n",
    "            print(f\"\\n✓ Saved to: {output_path}\")\n",
    "            print(f\"✓ Shape: {features.shape}\")\n",
    "            print(f\"✓ Features per window: {features.shape[1] - 3}\")\n",
    "    else:\n",
    "        print(\"No valid files found. Please check file paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
